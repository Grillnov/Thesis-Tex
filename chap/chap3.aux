\relax 
\citation{Caffeine}
\@writefile{toc}{\contentsline {chapter}{\numberline {第三章\hspace  {0.3em}}卷积神经网络}{9}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}卷积层}{9}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces 卷积层的计算过程 }}{10}{figure.3.1}}
\newlabel{Convolution}{{3.1}{10}{卷积层的计算过程\relax }{figure.3.1}{}}
\citation{AlexNet}
\citation{FPGA}
\citation{AlexNet}
\citation{CIFAR}
\citation{CIFAR}
\citation{ReLU}
\citation{AlexNet}
\citation{AlexNet}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}ReLU激活函数}{11}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces 不同激活函数的收敛速度。本图反应一个四层卷积神经网络在CIFAR-10\hspace  {0.25em plus 0.125em minus 0.08em}\ignorespaces \textsuperscript  {\cite  {CIFAR}}数据集上的训练收敛过程，横轴为训练集所有图片的训练迭代世代数，纵轴为模型的错误率。 实线代表激活函数为ReLU，虚线代表激活函数为$tanh$。由图可见ReLU的收敛速度快6倍左右。 }}{12}{figure.3.2}}
\newlabel{Epochs}{{3.2}{12}{不同激活函数的收敛速度。本图反应一个四层卷积神经网络在CIFAR-10~\supercite {CIFAR}数据集上的训练收敛过程，横轴为训练集所有图片的训练迭代世代数，纵轴为模型的错误率。 实线代表激活函数为ReLU，虚线代表激活函数为$tanh$。由图可见ReLU的收敛速度快6倍左右。\relax }{figure.3.2}{}}
\citation{AlexNet}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}池化层}{13}{section.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}全连接层}{13}{section.3.4}}
\@setckpt{chap/chap3}{
\setcounter{page}{14}
\setcounter{equation}{1}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{CTEX@sectiondepth}{2}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{2}
\setcounter{table}{0}
\setcounter{lstnumber}{1}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{8}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
